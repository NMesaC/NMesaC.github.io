<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Finetuning FROMAGE with Pokemon | Nicholas Mesa-Cucalon </title> <meta name="author" content="Nicholas Mesa-Cucalon"> <meta name="description" content="A website for my ideas and projects! "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, Nicholas-Mesa-Cucalon, MultimodalML, Multimodal-Machine-Learning, Personalizable-AI, Generative-AI, GenerativeAI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.jpg?bae2317adc46646ad196db4b2f8c58ac"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nmesac.github.io/projects/1_scizor_finetuning/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Nicholas</span> Mesa-Cucalon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/summaries/">Paper Summaries </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Finetuning FROMAGE with Pokemon</h1> <p class="post-description"></p> </header> <article> <h2 id="motivation">Motivation</h2> <p>One of the key objectives in current Machine Learning research is the desire to combine different modalities for a more unified representation concept. A model that can process and perform inference with both language and vision data will inevitably be far more powerful than what a language or vision model can do in a vacuum. This idea is called Grounding in the literature. Grounding is the idea of having a model being able to understand multiple representations of the same concept. Consider a pre-trained language model. The model might understand that an apple is a red fruit, but we want a model that can go further. We want a model that can also be able to process an image of an apple and recognize that the features of the image are the same features that, in natural language, are unique to an apple. It is clear why such a model would be useful, as you are in essence giving a large language model “eyes”. Claude 3.5 Sonnet and ChatGPT4-o have recently been able to accomplish this. They are able to process and discuss images when prompted to. Specifically, the vision capabilities of both these models are impressive. I prompted Claude with the question “Describe this character” on the image below, Claude provided an accurate description of the image that couldn’t have been mistaken for another character.</p> <div style="text-align:center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/scizor_images/complete_nahobino.webp" sizes="95vw"></source> <img src="/assets/img/scizor_images/complete_nahobino.webp" class="z-depth-1" width="225px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Furthermore, this model is definitely out of the scope of Claude’s training, as this image only became publicly available around early to mid June (the release date of the game the character was from), so being able to provide such an accurate natural language description of a novel image was amazing. Once I saw these capabilities, I became interested in understanding grounding more and seeing if I could replicate the results, albeit at a much smaller scale. I had already read and summarized the paper “Grounding Language models to Images for Multimodal Inputs and Outputs” and I figured that getting more experience working with the FROMAGE framework described in the paper would be good practice. I selected to finetune FROMAGE on Pokemon because I’m a big fan of the series and also I thought that Pokemon would give me good practice describing non standard objects in natural language.</p> <h2 id="scope-and-data">Scope and Data</h2> <p>When I was scoping out this project, I originally wanted to choose a select number of the best Pokemon currently so that the model could identify them via image and then give me some information about them. However, I quickly realized upon searching that there are no robust datasets for this type of task with Pokemon. The best that exists is a kaggle dataset that contains each of the roughly 1000 Pokemon and a general description of the Pokemon. The issue is that there is only 1 image of each Pokemon, and it gives a general natural language description of each Pokemon, which is insufficient for what I wanted to do. I realized that I either would have to change what my dataset would be or I would have to make a smaller scale one for myself. I then decided that I needed to shift scope and decided to focus on a single Pokemon and make a small training dataset for it. I decided I would use Scizor, as I feel this Pokemon has distinctive enough features that would be easy for the model to reason about.</p> <div style="text-align:center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/scizor_images/scizor-480.webp 480w,/assets/img/scizor_images/scizor-800.webp 800w,/assets/img/scizor_images/scizor-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/scizor_images/scizor.png" class="z-depth-1" width="225px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>To create a small dataset for this Pokemon, I need to write a web scraper, which was my first major roadblock. Google changed the way usual web scraping packages (such as Selenium) can interact with it to make queries, which lost me a lot of time trying to figure out workarounds. In the end, I just used Serpapi, which through a single API call, would perform all the requests for me. I spent a lot of time trying to come up with a general script that could be changed in the future to collect data without restriction, but I yielded so I could finish the project in a timely manner. Serpapi has a limit of 100 free API calls a month, which is more than sufficient for most use cases, so I was grateful for that at least. Another issue was with how I needed to annotate the data. It would defeat the point of this project to just use GPT4-o or Claude API calls to generate natural language descriptions of the Scizor images, so I decided to instead label the images by hand. This took quite a bit of time, and since I did it in the Conceptual Captions (cc3m) dataset style, using natural language annotations that were diverse and still descriptive was far more difficult than anticipated. I learned an important lesson quickly with this project, and that’s how necessary good, high quality data for Machine Learning models is at this scale. I only labeled about 70 images with natural language descriptions, so I only had a small amount of data to work with. I did this since I wanted to get to work with FROMAGE as soon as possible, so I made do with what data I had, though it did impact the results later.</p> <h2 id="evaluation">Evaluation</h2> <p>Once I was able to get FROMAGE to actually load the model and evaluate, I encountered something I suspected might happen from the web scraping phase. The model only had limited training data to learn from, and since I did not employ any other advanced techniques to supplement cases with few examples, the model performance was mediocre on most tasks related to Scizor specifically, as it still retained its capabilities from beforehand. The model is able to describe an image properly, but the reasoning we expect from LLM’s is not fully there with reference to Scizor. Of course, I am working on a much smaller scale than models like Claude or ChatGPT4-o, but I still expected slightly better results. I suspect that with more training data and more time, the model will be able to perform better on more complicated and less direct queries.</p> <div style="text-align:center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/scizor_images/fromage_out-480.webp 480w,/assets/img/scizor_images/fromage_out-800.webp 800w,/assets/img/scizor_images/fromage_out-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/scizor_images/fromage_out.png" class="z-depth-1" width="600px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="future-work">Future Work</h2> <p>The fine tuning procedure worked in the sense that the model can actually recognize Scizor and answer basic queries, but it failed since the model cannot sustain a full conversation about Scizor as the original paper showed with a cat or a beaver as a prompt. I attribute this to the lack of data regarding Scizor in the Conceptual Captions style. I always learned in classes the necessity of good data for a Machine Learning model to actually perform effectively, but I experienced it first hand with this project. I feel that I learned some useful skills throughout this project. Firstly, I only have access to a Macbook Air, which definitely cannot perform training fast enough for this model nor can it fit the model in its RAM. I ended up getting familiar with the LambdaLabs GPU services, setting up environments on fresh Ubuntu installs and just getting comfortable with the product, which I know will be useful later on. I also learned about web scraping and parsing data from Serpapi, which will inevitably be useful when I do later projects in Multimodal contexts. Finally, working with FROMAGE helped me work with research code and debugging it in an ML context. I have done debugging of research code and reading through github and stackoverflow posts from my internship during Summer 2023, but doing it with pytorch instead was a new experience, and I’m grateful for that. In the future, I might revisit this project and either use an existing multimodal model to automate image captioning, expanding the existing Scizor dataset or using a different dataset all together. Regardless, I learned a lot and enjoyed working on this project, and the paper and this model have given me ideas for future projects!</p> <h2 id="note-procedures">Note: Procedures</h2> <p>Full details of the scripts I used to prepare the demo are available at <a href="https://github.com/NMesaC/fromage_finetuning_pokemon" rel="external nofollow noopener" target="_blank"> my github! </a></p> <div style="text-align:center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/scizor_images/scizor_end-480.webp 480w,/assets/img/scizor_images/scizor_end-800.webp 800w,/assets/img/scizor_images/scizor_end-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/scizor_images/scizor_end.jpg" class="z-depth-1" width="600px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Nicholas Mesa-Cucalon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>